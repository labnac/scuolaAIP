{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/labnac/scuolaAIP/blob/main/2_keras_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UZE_Y7m4M2v"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.datasets import load_wine\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import genfromtxt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "\n",
        "def create_custom_model(input_dim, output_dim, nodes, layers=1, name='model'):\n",
        "    # Create model\n",
        "    model = Sequential(name=name)\n",
        "    for i in range(layers):\n",
        "      if i == 1:\n",
        "        model.add(Dense(nodes[i], input_dim=input_dim, activation='linear'))\n",
        "      else:\n",
        "        model.add(Dense(nodes[i], input_dim=input_dim, activation='sigmoid'))\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='mean_squared_error',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['RootMeanSquaredError'])\n",
        "    return model\n",
        "\n",
        "wine = load_wine()\n",
        "features = wine['data']\n",
        "target = wine['target']\n",
        "names = wine['target_names']\n",
        "feature_names = wine['feature_names']\n",
        "enc = OneHotEncoder()\n",
        "target = enc.fit_transform(target[:, np.newaxis]).toarray()\n",
        "\n",
        "# Scale data to have mean 0 and variance 1\n",
        "# which is importance for convergence of the neural network\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "# Split the data set into training and testing\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    features_scaled, target, test_size=0.2, random_state=2)\n",
        "\n",
        "n_features = features.shape[1]\n",
        "n_classes = target.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "n_layers = 1  # must be greater than 0\n",
        "n_hidden_neurons = 8\n",
        "model = create_custom_model(n_features, n_classes, n_hidden_neurons, n_layers)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "history_dict = {}\n",
        "\n",
        "# TensorBoard Callback\n",
        "cb = TensorBoard()\n",
        "\n",
        "\n",
        "history_callback = model.fit(X_train, Y_train,\n",
        "                              batch_size=32,\n",
        "                              epochs=200,\n",
        "                              verbose=1,\n",
        "                              validation_data=(X_test, Y_test),\n",
        "                              callbacks=[cb])\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "print(history_callback)\n",
        "history_dict[model.name] = [history_callback, model]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, figsize=(8, 6))\n",
        "\n",
        "for model_name in history_dict:\n",
        "    val_accuracy = history_dict[model_name][0].history['val_accuracy']\n",
        "    val_loss = history_dict[model_name][0].history['val_loss']\n",
        "    accuracy = history_dict[model_name][0].history['accuracy']\n",
        "    loss = history_dict[model_name][0].history['loss']\n",
        "    ax1.plot(val_accuracy, label=model_name + \" val_accuracy\")\n",
        "    ax1.plot(accuracy, label=model_name + \" accuracy\")\n",
        "    ax2.plot(val_loss, label=model_name + \" val loss\")\n",
        "    ax2.plot(loss, label=model_name + \" loss\")\n",
        "\n",
        "ax1.set_ylabel('validation accuracy')\n",
        "ax2.set_ylabel('validation loss')\n",
        "ax2.set_xlabel('epochs')\n",
        "ax1.legend()\n",
        "ax2.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ]
}