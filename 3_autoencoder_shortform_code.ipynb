{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjJJr46Tj8Q3Fi37FuNrAl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/labnac/scuolaAIP/blob/main/3_autoencoder_shortform_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Automatic creation of short-forms with Autoencoders**"
      ],
      "metadata": {
        "id": "Yus5ZJzkTwwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import dei moduli"
      ],
      "metadata": {
        "id": "HTsMQHcgs8Ca"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGBFSYIeoolH"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import keras\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras import regularizers, optimizers, callbacks\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definizione dell'autoencoder"
      ],
      "metadata": {
        "id": "7-KDuNl5s_Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "\n",
        "\n",
        "def create_autoencoder(input_dim, l_rate, name='autoencoder'):\n",
        "    # Create model\n",
        "    autoencoder = Sequential(name=name)\n",
        "\n",
        "    # Encoder\n",
        "    autoencoder.add(Dense(5, input_dim=input_dim, activation='sigmoid'))\n",
        "\n",
        "    # Internal layer\n",
        "    autoencoder.add(Dense(3, activation='linear'))\n",
        "\n",
        "    # Decoder\n",
        "    autoencoder.add(Dense(5, activation='sigmoid'))\n",
        "\n",
        "    # Output layer\n",
        "    autoencoder.add(Dense(input_dim, activation='linear'))\n",
        "\n",
        "    # Compile model\n",
        "    autoencoder.compile(loss='mean_squared_error',\n",
        "                        optimizer=tf.keras.optimizers.Adam(learning_rate = l_rate))\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "def get_output(data):\n",
        "    new_model = tf.keras.Model(inputs=autoencoder_model.input, outputs=autoencoder_model.output, verbose = 0)\n",
        "    output_layer = new_model.predict(data)\n",
        "    return output_layer"
      ],
      "metadata": {
        "id": "zDN6ANjLowDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = np.loadtxt(\"Heterogeneous_1000_1.txt\", skiprows=1, delimiter=\",\")"
      ],
      "metadata": {
        "id": "pt2b9G00qh4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = Dataset.shape[1]\n",
        "\n",
        "l_rate = 5e-3\n",
        "\n",
        "autoencoder_model = create_autoencoder(n_features, l_rate)\n",
        "\n",
        "autoencoder_model.summary()\n",
        "\n",
        "history_dict = {}\n",
        "\n",
        "# TensorBoard Callback\n",
        "cb = TensorBoard()\n",
        "\n",
        "history_callback = autoencoder_model.fit(Dataset, Dataset,\n",
        "                              batch_size=4,\n",
        "                              epochs=50,\n",
        "                              verbose=1,\n",
        "                              callbacks=[cb])\n",
        "\n",
        "#mse_autoencoder = history_callback.history['mean_squared_error'][-1]\n",
        "\n",
        "score = autoencoder_model.evaluate(Dataset, Dataset, verbose=0)\n",
        "\n",
        "print(history_callback)\n",
        "history_dict[autoencoder_model.name] = [history_callback, autoencoder_model]"
      ],
      "metadata": {
        "id": "FnoaUpjNsepB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot dell'addestramento\n",
        "loss = history_dict[autoencoder_model.name][0].history['loss']\n",
        "plt.plot(loss, label= autoencoder_model.name + \" loss\")\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs')"
      ],
      "metadata": {
        "id": "LPoTTSC9ILhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creo un vettore che va da 0 al numero di variabili\n",
        "variables_range = range(0,n_features)\n",
        "\n",
        "#produco una copia del dataset da cui andrò ad annullare le variabili\n",
        "Dataset_to_drop = np.copy(Dataset)\n",
        "\n",
        "#creo due liste che conterranno le variabili e gli errori di ricostruzione per ogni step della procedura\n",
        "ann_var = []\n",
        "min_MSE = []\n",
        "#inizializzo il primo loop che scorre su tutte le variabili. Questi sono gli step della procedura.\n",
        "for variable in variables_range:\n",
        "\n",
        "    MSE_Scores = [] #inizializzo una lista che conterrà gli errori per ogni variabile annullata ad ogni SINGOLO step\n",
        "\n",
        "    for iteration in variables_range: # loop per iterare su tutte le variabili AD OGNI STEP\n",
        "\n",
        "        mult_vector = np.ones(Dataset_to_drop.shape[1]) #definisco un vettore di 1 con tanti elementi quante sono le mie variabili\n",
        "\n",
        "        #se nella specifica iterazione il valore è ancora diverso da 0 (cioè non è stato già annullato nelle iterazioni precedenti)\n",
        "        if np.any(Dataset_to_drop[:, iteration] != 0):\n",
        "\n",
        "          np.put(mult_vector, [iteration], [0]) #lo pongo pari a 0 nel vettore che ho creato per aiutarmi\n",
        "\n",
        "        #ora posso moltiplicare il vettore alla matrice. in questo modo tutta la colonna della variabile sarà 0\n",
        "          X_Ann_Var = Dataset_to_drop*mult_vector\n",
        "\n",
        "        #passo alla rete nete neurale il dataset con la variabile annullata e ottengo in output la ricostruzione della long-form\n",
        "          reconstructed_array = get_output(X_Ann_Var)\n",
        "\n",
        "        #arrotondo l'output per avere numeri interi\n",
        "          rounded_array = np.rint(reconstructed_array)\n",
        "\n",
        "        #calcolo l'errore di ricostruzione\n",
        "          MSE = mean_squared_error(Dataset, rounded_array, squared = False)\n",
        "\n",
        "         #conservo l'errore di ricostruzione nella lista  creata nel loop\n",
        "          MSE_Scores.append(MSE)\n",
        "\n",
        "        # se invece la variabile è già stata annullata, inserirò al posto dell'errore un numero fittizio molto alto, es. 999\n",
        "        else:\n",
        "          MSE_Scores.append(999)\n",
        "\n",
        "    #a questo punto trasformo la lista di MSE in un array numpy per recuperare\n",
        "    #l'indice del suo minimo e contestualmente il numero dell'item a questo associato\n",
        "    MSE_Scores = np.array(MSE_Scores)\n",
        "\n",
        "    #aggiungo 1 per recuperare il numero preciso del mio item annullato perchè gli indici partono da 0\n",
        "    ann_var.append (np.argmin(MSE_Scores) + 1)\n",
        "    #a questo punto annullo ufficialmente la variabile nel mio dataset e ripeto tutto per le variabili rimanenti\n",
        "    #passando all'iterazione successiva\n",
        "    Dataset_to_drop[:, (np.argmin(MSE_Scores))] = 0\n",
        "\n",
        "    #conservo il minimo MSE di questa iterazione per controllare come varia l'errore tra le diverse iterazioni\n",
        "    min_MSE.append(np.amin(MSE_Scores))"
      ],
      "metadata": {
        "id": "nN4P8flPMkSv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}